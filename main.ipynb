{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Kernel Approximation\n",
    "\n",
    "1. Environment Setup: import required libraries\n",
    "2. Loading Dataset: training set, validation set, test set\n",
    "3. Preprocessing Data\n",
    "4. Undersampling Data\n",
    "5. Concatenating Data\n",
    "6. Building the Model\n",
    "7. Training Data\n",
    "8. Evalutaing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup: import required library\n",
    "\n",
    "We include the required libraries that will be used in the next parts. The **time**, **numpy**, and **tensorflow** are common libraries in machine learning. The **fio**, which is \"file input output\" used to load data and **config**, which is \"configuration file\" used to config the path of the dataset files are written by myself. Modify it when you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fio\n",
    "from config import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Dataset: training set, validation set, test set\n",
    "\n",
    "Loading the training set, validation set that was defined in **config.py** file. The sample files consisting of indices of data will be used to undersample the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = fio.load_file(train_data_dict['fcsv_phs'])\n",
    "Y_train_orig = fio.load_file(train_data_dict['tcsv_phs'])\n",
    "X_valid_orig = fio.load_file(eval_data_dict['fcsv_phs'])\n",
    "Y_valid_orig = fio.load_file(eval_data_dict['tcsv_phs'])\n",
    "train_sample = fio.load_sample_file(train_dataset_dict['Short-TrainSet-UdrSamp-3_3_1p0_1p0_0p1'])\n",
    "valid_sample = fio.load_sample_file(valid_dataset_dict['Short-ValidSet-NoUdrSamp'])\n",
    "\n",
    "print(\"the length of training set:\", len(X_train_orig))\n",
    "print(\"the length of evaluating set:\", len(X_valid_orig))\n",
    "print(\"the first row of training data:\", X_train_orig[0][0][0])\n",
    "print(\"the target value:\", Y_train_orig[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data\n",
    "\n",
    "There are two parts of the data that must be processed:\n",
    "\n",
    "- the input data represented by the prefix \"**X**\" on variables\n",
    "- the target data represented by the prefix \"**Y**\" on variables\n",
    "\n",
    "The data will be processed on the following steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Preprocessing Input Data\n",
    "\n",
    "Dealing with the input data has three steps:\n",
    "\n",
    "1. Calculating the feature statistic data\n",
    "2. Standardize the data\n",
    "3. Expand the features by window slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Calculating the feature statistic data\n",
    "\n",
    "we will write a function to calculate the training and validating data. The mean, maximum, standard deviation, and variance are returned at the end of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_stat(arr):\n",
    "    arr = [ f.reshape((-1, f.shape[-1])) for f in arr ]\n",
    "    arr = np.concatenate(arr)\n",
    "\n",
    "    return {\n",
    "            'max': np.max(arr, axis=0),\n",
    "            'mean': np.mean(arr, axis=0, dtype=np.float128),\n",
    "            'stdev': np.nanstd(arr, axis=0, dtype=np.float128),\n",
    "            'var': np.nanvar(arr, axis=0, dtype=np.float128),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = get_feat_stat(X_train_orig + X_valid_orig)\n",
    "print(\"maximum:\", stat['max'])\n",
    "print(\"mean:\", stat['mean'])\n",
    "print(\"standard deviation:\", stat['stdev'])\n",
    "print(\"variance:\", stat['var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Standardize the data\n",
    "\n",
    "Next, we will standardize the data by the **stat** calculated from the previous step. \n",
    "\n",
    "How to standardize the data: https://stackoverflow.com/a/4544459\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(A, stat):\n",
    "    if isinstance(A, list):\n",
    "        return [ standardize(a, stat) for a in A ]\n",
    "\n",
    "    A = np.subtract(A, stat['mean'])\n",
    "    A = np.divide(A, stat['stdev'])\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before standardization:\", X_train_orig[0][0][0])\n",
    "X_train_orig = standardize(X_train_orig, stat)\n",
    "X_valid_orig = standardize(X_valid_orig, stat)\n",
    "print(\"after standardization:\", X_train_orig[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Expand the features by window slice\n",
    "\n",
    "How to expand the features by window size: https://zhuanlan.zhihu.com/p/64933417\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(A, window_size):\n",
    "    if not window_size & 0x1:\n",
    "        raise Exception('need odd value on padding')\n",
    "\n",
    "    if isinstance(A, list):\n",
    "        return [ expand(a, window_size) for a in A ]\n",
    "\n",
    "    # For example\n",
    "    # A is a (5,3,6) matrix\n",
    "    # window_size is 5\n",
    "    # \n",
    "    # A = np.arange(0,5*3*6,1).reshape((5,3,6)).astype(np.float128)\n",
    "    # A = np.pad(A, ((2,2), (2,2), (0,0)), mode='constant')\n",
    "    # A = strided(A, shape=(5,3,5,5,6), strides=(672,96,672,96,16))\n",
    "    # A = A.reshape((5,3,150))\n",
    "    #\n",
    "    # For more information:\n",
    "    # https://zhuanlan.zhihu.com/p/64933417\n",
    "\n",
    "    n = window_size # the height and width of the window\n",
    "    p = window_size >> 1 # the padding size\n",
    "\n",
    "    d0, d1, d2 = A.shape # dimansion 0, 1, 2\n",
    "    s0, s1, s2 = A.strides # stride 0, 1, 2\n",
    "\n",
    "    A = np.pad(A, pad_width=((p,p),(p,p),(0,0)), mode='constant')\n",
    "    A = np.lib.stride_tricks.as_strided(A, shape=(d0,d1,n,n,d2), strides=(s0,s1,s0,s1,s2))\n",
    "    A = A.reshape((d0,d1,d2*n*n))\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "X_train = expand(X_train_orig, window_size)\n",
    "X_valid = expand(X_valid_orig, window_size)\n",
    "print(\"before expand:\", X_train_orig[0].shape)\n",
    "print(\"after expand:\", X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Preprocessing Output Data\n",
    "\n",
    "The target will be classified into two categories. \n",
    "\n",
    "- the target value is zero \n",
    "- the target vlaue is not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(A):\n",
    "    if isinstance(A, list):\n",
    "        return [ classify(a) for a in A ]\n",
    "\n",
    "    A = A != 0\n",
    "    A = A.astype(int)\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = classify(Y_train_orig)\n",
    "Y_valid = classify(Y_valid_orig)\n",
    "print(\"before classification:\", Y_train_orig[0][0][0])\n",
    "print(\"after classification:\", Y_train[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Undersampling Data\n",
    "\n",
    "Because of the large data, we need to undersample the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(arr, idx):\n",
    "    if isinstance(arr, list):\n",
    "        return [undersample(f, i) for f, i in zip(arr, idx)]\n",
    "    return arr[idx[:,0],idx[:,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the size of training sample:\", len(train_sample[0]))\n",
    "print(\"the first element of training sample:\", train_sample[0][0])\n",
    "print(\"before undersampling shape:\", X_train[0].shape)\n",
    "X_train = undersample(X_train, train_sample)\n",
    "X_valid = undersample(X_valid, valid_sample)\n",
    "Y_train = undersample(Y_train, train_sample)\n",
    "Y_valid = undersample(Y_valid, valid_sample)\n",
    "print(\"after undersampling shape:\", X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Concatenating Data\n",
    "\n",
    "The type of list cannot be trained by tensorflow, so we need to convert data from a **list** to a **numpy array**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the type before concatenation:\", type(X_train))\n",
    "X_train = np.concatenate(X_train).astype(np.float32)\n",
    "X_valid = np.concatenate(X_valid).astype(np.float32)\n",
    "Y_train = np.concatenate(Y_train)\n",
    "Y_valid = np.concatenate(Y_valid)\n",
    "print(\"the type after concatenation:\", type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building the Model\n",
    "\n",
    "The model that we used is followed by the article: [Improving Linear Models Using Explicit Kernel Methods](https://github.com/Debian/tensorflow/blob/master/tensorflow/contrib/kernel_methods/g3doc/tutorial.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learining_rate = 50.0\n",
    "l2_regularization_strength = 0.001\n",
    "\n",
    "# Random Fourier Feature Mapper\n",
    "dim_in  = window_size * window_size * 6\n",
    "dim_out = window_size * window_size * 6 * 10\n",
    "stddev  = 5.0\n",
    "\n",
    "optimizer = tf.train.FtrlOptimizer(learning_rate=learining_rate, l2_regularization_strength=l2_regularization_strength)\n",
    "\n",
    "image_column = tf.contrib.layers.real_valued_column('data', dimension=dim_in)\n",
    "kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim=dim_in, output_dim=dim_out, stddev=stddev, name='rffm')\n",
    "\n",
    "estimator = tf.contrib.kernel_methods.KernelLinearClassifier(n_classes=2, optimizer=optimizer, kernel_mappers={image_column: [kernel_mapper]})\n",
    "\n",
    "# For Example: Linear Model without optimizer\n",
    "# image_column = tf.contrib.layers.real_valued_column('data', dimension=784)\n",
    "# estimator = tf.contrib.learn.LinearClassifier(feature_columns=[image_column], n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "epoch = 1\n",
    "steps = 2000\n",
    "\n",
    "x = {'data':X_train}\n",
    "y = Y_train\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, batch_size=batch, shuffle=False, num_epochs=epoch)\n",
    "\n",
    "# Train.\n",
    "start = time.time()\n",
    "estimator.fit(input_fn=train_input_fn, steps=2000)\n",
    "end = time.time()\n",
    "print('Elapsed time: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evalutaing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'data':X_valid}\n",
    "y = Y_valid\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, batch_size=2, shuffle=False, num_epochs=1)\n",
    "\n",
    "# Evaluate and report metrics.\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "print(eval_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
