{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Kernel Approximation\n",
    "\n",
    "1. Environment Setup: import required libraries\n",
    "2. Loading Dataset: training set, validation set, test set\n",
    "3. Preprocessing Data\n",
    "4. Building the Model\n",
    "5. Training Data\n",
    "6. Validation Data\n",
    "7. Testing Data\n",
    "\n",
    "Time: O(n ^ 2) ~ O(n ^ 3)\n",
    "Memory: O(n * w * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup: import required library\n",
    "\n",
    "We include the required libraries that will be used in the next parts. The **time**, **numpy**, and **tensorflow** are common libraries in machine learning. The **fio**, which is \"file input output\" used to load data and **config**, which is \"configuration file\" used to config the path of the dataset files are written by myself. Modify it when you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fio\n",
    "import preprocessing as pc\n",
    "from config import *\n",
    "\n",
    "# python std library\n",
    "import gc\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# install library\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Dataset: training set, validation set, test set\n",
    "\n",
    "Loading the training set, validation set, and test set that was defined in **config.py** file. The sample files consisting of indices of data will be used to undersample the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = fio.load_file(train_data_dict['fcsv_phs'])\n",
    "Y_train_orig = fio.load_file(train_data_dict['tcsv_phs'])\n",
    "X_test_orig = fio.load_file(eval_data_dict['fcsv_phs'])\n",
    "Y_test_orig = fio.load_file(eval_data_dict['tcsv_phs'])\n",
    "\n",
    "print(\"the length of training set:\", len(X_train_orig))\n",
    "print(\"the length of testing set:\", len(X_test_orig))\n",
    "print(\"the first row training data:\", X_train_orig[0][0][0])\n",
    "print(\"the first row target value:\", Y_train_orig[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data\n",
    "\n",
    "There are two parts of the data that must be processed:\n",
    "\n",
    "- the input data represented by the prefix \"**X**\" on variables\n",
    "- the target data represented by the prefix \"**Y**\" on variables\n",
    "\n",
    "The data will be processed on the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X, Y, statistic, window_size=1, sample=None):\n",
    "    \n",
    "    w = window_size\n",
    "    dx = X.shape[2]\n",
    "    dy = Y.shape[2]\n",
    "    \n",
    "    X = pc.standardize(X, statistic)\n",
    "    X = pc.expand(X, window_size)\n",
    "    \n",
    "    Y = pc.classify(Y)\n",
    "    \n",
    "    if sample is None:\n",
    "        X = X.reshape((-1, w, w, dx))\n",
    "        Y = Y.reshape((-1, dy))\n",
    "    else:\n",
    "        X = pc.undersample(X, sample)\n",
    "        Y = pc.undersample(Y, sample)\n",
    "    \n",
    "    return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_lists, Y_lists, statistic, window_size=1, samples=[]):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for x, y, s in itertools.zip_longest(X_lists, Y_lists, samples):\n",
    "        d0, d1, _ = x.shape\n",
    "        print(\"data size:\",   d0*d1, x.shape, \n",
    "              \"sample size:\", d0*d1 if s is None else len(s))\n",
    "        x, y = pipeline(x, y, statistic, window_size, s)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        \n",
    "    X = np.concatenate(X, axis=0)\n",
    "    Y = np.concatenate(Y, axis=0)\n",
    "    \n",
    "    X = X.reshape((X.shape[0],-1))\n",
    "    Y = Y.reshape((Y.shape[0],-1))\n",
    "    \n",
    "    return (X, Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 23 # window size\n",
    "stat = pc.get_feat_stat(X_train_orig)\n",
    "\n",
    "train_sample = fio.load_sample_file(train_dataset_dict['Short-TrainSet-UdrSamp-3_3_1p0_1p0_0p1'])\n",
    "X_train, Y_train = preprocessing(X_train_orig, Y_train_orig, stat, window_size=w, samples=train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Model\n",
    "\n",
    "The model that we used is followed by the article: [Improving Linear Models Using Explicit Kernel Methods](https://github.com/Debian/tensorflow/blob/master/tensorflow/contrib/kernel_methods/g3doc/tutorial.md).\n",
    "\n",
    "https://storage.googleapis.com/pub-tools-public-publication-data/pdf/18d86099a350df93f2bd88587c0ec6d118cc98e7.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learining_rate = 50.0\n",
    "l2_regularization_strength = 0.001\n",
    "\n",
    "# Random Fourier Feature Mapper\n",
    "dim_in  = w * w * 6\n",
    "dim_out = w * w * 6 * 10\n",
    "stddev  = 5.0\n",
    "\n",
    "optimizer = tf.train.FtrlOptimizer(learning_rate=learining_rate, l2_regularization_strength=l2_regularization_strength)\n",
    "\n",
    "image_column = tf.contrib.layers.real_valued_column('data', dimension=dim_in)\n",
    "kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim=dim_in, output_dim=dim_out, stddev=stddev, name='rffm')\n",
    "\n",
    "estimator = tf.contrib.kernel_methods.KernelLinearClassifier(n_classes=2, optimizer=optimizer, kernel_mappers={image_column: [kernel_mapper]})\n",
    "\n",
    "# For Example: Linear Model without optimizer\n",
    "# image_column = tf.contrib.layers.real_valued_column('data', dimension=784)\n",
    "# estimator = tf.contrib.learn.LinearClassifier(feature_columns=[image_column], n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = 2\n",
    "epoch = 1\n",
    "steps = 2000\n",
    "\n",
    "x = {'data':X_train}\n",
    "y = Y_train\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, batch_size=batch, shuffle=False, num_epochs=epoch)\n",
    "# train_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, shuffle=False)\n",
    "\n",
    "\n",
    "# Train.\n",
    "start = time.time()\n",
    "estimator.fit(input_fn=train_input_fn, steps=steps)\n",
    "end = time.time()\n",
    "print('Elapsed time: {} seconds'.format(end - start))\n",
    "\n",
    "eval_metrics = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "print(\"train data evaluated matrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, Y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_sample = fio.load_sample_file(valid_dataset_dict['Short-ValidSet-NoUdrSamp'])\n",
    "X_valid, Y_valid = preprocessing(X_train_orig, Y_train_orig, stat, window_size=w, samples=valid_sample)\n",
    "\n",
    "x = {'data':X_valid}\n",
    "y = Y_valid\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(Y_valid.shape)\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, batch_size=2, shuffle=False, num_epochs=1)\n",
    "# eval_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, shuffle=False)\n",
    "\n",
    "\n",
    "# Evaluate and report metrics.\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "print(\"validation data evaluated matrics:\", eval_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_valid, Y_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = preprocessing(X_test_orig, Y_test_orig, stat, window_size=w)\n",
    "\n",
    "x = {'data':X_test}\n",
    "y = Y_test\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "# eval_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, batch_size=2, shuffle=False, num_epochs=1)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x, y, shuffle=False)\n",
    "\n",
    "# Evaluate and report metrics.\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "print(\"validation data evaluated matrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
